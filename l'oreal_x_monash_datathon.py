# -*- coding: utf-8 -*-
"""L'Oreal x Monash Datathon.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NswqU3MuGEvx6HAvRzNf62pftuWIIA7Y
"""

!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/comments1.csv?download=true -O comments1.csv
!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/videos.csv?download=true -O videos.csv
!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/comments2.csv?download=true -O comments2.csv
!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/comments3.csv?download=true -O comments3.csv
!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/comments4.csv?download=true -O comments4.csv
!wget https://huggingface.co/datasets/Dickson18643/LOrealxMonashDatathon/resolve/main/comments5.csv?download=true -O comments5.csv

import os
import pandas as pd
import numpy as np

base_path = "./"

# List of file paths
file_paths = [
    base_path + "comments1.csv",
    base_path + "comments2.csv",
    base_path + "comments3.csv",
    base_path + "comments4.csv",
    base_path + "comments5.csv",
    base_path + "videos.csv"
]

# Read all files into a list of DataFrames
dataframes = []
for file_path in file_paths:
    df = pd.read_csv(file_path)
    dataframes.append(df)
    print(f"Loaded {os.path.basename(file_path)} with shape {df.shape}")

# Access individual DataFrames
comm_df1, comm_df2, comm_df3, comm_df4, comm_df5, video_df = dataframes

comm_df = pd.concat([comm_df1, comm_df2, comm_df3, comm_df4, comm_df5], ignore_index = True)

comm_df["updatedAt"] = pd.to_datetime(comm_df["updatedAt"])

top_500_recent = comm_df.sort_values(by="updatedAt", ascending=False).head(500)
print(top_500_recent[["textOriginal","updatedAt"]].head(20))
top_500_recent.to_csv("top_500_recent_comments.csv", index = False, encoding = "utf-8-sig")

top_recent_comments = pd.read_excel("top_500_recent_comments.xlsx")
top_recent_comments = top_recent_comments.drop(top_recent_comments.columns[-1], axis = 1)
top_recent_comments.dtypes

import glob
from tqdm import tqdm  # For progress bars

def load_and_merge_data(top_recent_comments="top_500_recent_comments.xlsx", video_file='videos.csv'):
    top_recent_comments = pd.read_excel(top_recent_comments)
    top_recent_comments = top_recent_comments.drop(top_recent_comments.columns[-1], axis = 1)

    video_df = pd.read_csv(video_file)

    merged_df = pd.merge(
        top_recent_comments,
        video_df[['videoId', 'title', 'viewCount', 'likeCount','description','tags','contentDuration']],
        on='videoId',
        how='left'
    )

    merged_df = merged_df.rename(columns = {
        "likeCount_x": "commentLikeCount",
        "likeCount_y": "videoLikeCount"
    })

    return merged_df

# Load your data
print("Loading and merging data...")
top_recent_comments = load_and_merge_data()
print(f"Loaded {len(top_recent_comments)} comments with video metadata")

def handle_missing_data(df):
    df_clean = df.copy()

    df_clean = df_clean.dropna(subset=['textOriginal'])
    df_clean['parentCommentId'] = df_clean['parentCommentId'].fillna(0)
    df_clean[['title','description','tags']] = df_clean[['title','description','tags']].fillna('')
    df_clean["viewCount"] = df_clean["viewCount"].fillna(0).astype("int64")
    df_clean['videoLikeCount'] = df_clean["videoLikeCount"].fillna(0).astype("int64")
    df_clean['contentDuration'] = df_clean['contentDuration'].fillna('PT0M0S')

    print("\nMissing values after handling:")
    print(df_clean.isna().sum())
    print(f"\nRemaining rows: {len(df_clean)}")

    return df_clean

top_recent_comments = handle_missing_data(top_recent_comments)

pip install emoji deep_translator

import os
from sentence_transformers import SentenceTransformer, util
from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, pipeline
import emoji
import torch
from deep_translator import GoogleTranslator

class CommentQualityAnalyzer:
  def __init__(self):
    if not os.path.exists("./model"):
      os.system("mkdir model")
      self.download_model()

  def download_model(self):
    SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2").save("./model/all-MiniLM-L6-v2")
    AutoModelForSequenceClassification.from_pretrained("AntiSpamInstitute/spam-detector-bert-MoE-v2.2").save_pretrained("./model/spam-detector-bert-MoE-v2.2")
    AutoTokenizer.from_pretrained("AntiSpamInstitute/spam-detector-bert-MoE-v2.2").save_pretrained("./model/spam-detector-bert-MoE-v2.2")
    AutoModelForSequenceClassification.from_pretrained("tabularisai/multilingual-sentiment-analysis").save_pretrained("./model/multilingual-sentiment-analysis")
    AutoTokenizer.from_pretrained("tabularisai/multilingual-sentiment-analysis").save_pretrained("./model/multilingual-sentiment-analysis")

  def deemojize(self, text):
    return emoji.demojize(str(text), delimiters=(" (", ") "))

  def semantic_similarity(self, title, comment):
    model = SentenceTransformer('./model/all-MiniLM-L6-v2')
    title_emb = model.encode(title)
    com_emb = model.encode(comment)
    return util.dot_score(title_emb, com_emb)[0].cpu().tolist()[0]

  def not_spam_prob(self, comment):
    tokenizer = AutoTokenizer.from_pretrained("./model/spam-detector-bert-MoE-v2.2")
    model = AutoModelForSequenceClassification.from_pretrained("./model/spam-detector-bert-MoE-v2.2")
    inputs = tokenizer(comment, return_tensors="pt")
    with torch.no_grad():
      outputs = model(**inputs)
      logits = outputs.logits
    return torch.softmax(logits, dim=1)[0][0].item()

  def sentiment_analysis(self, comment, weights=[-2,-1,0,1,2]):
    tokenizer = AutoTokenizer.from_pretrained("./model/multilingual-sentiment-analysis")
    model = AutoModelForSequenceClassification.from_pretrained("./model/multilingual-sentiment-analysis")
    inputs = tokenizer(comment, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    sentiment_map = {0: "Very Negative", 1: "Negative", 2: "Neutral", 3: "Positive", 4: "Very Positive"}
    label = {"Very Negative":weights[0], "Negative":weights[1], "Neutral":weights[2], "Positive":weights[3], "Very Positive":weights[4]}
    return label[sentiment_map[torch.argmax(probabilities, dim=-1).item()]]

  def translate_to_english(self, text):
      return GoogleTranslator(source="auto", target="en").translate(text)

  def engagement_ratio(self, like, view):
    return like/view

  def evaluate(self, title, comment, like, view, weights=[0.5,2,0.3,3], sentiment_weights=[-2,-1,0,1,2]):
    alpha = weights[0]
    beta = weights[1]
    gamma = weights[2]
    phi = weights[3]

    title = self.translate_to_english(self.deemojize(title))
    comment = self.translate_to_english(self.deemojize(comment))

    return alpha*self.semantic_similarity(title, comment) + beta*self.not_spam_prob(comment) + gamma*self.sentiment_analysis(comment, sentiment_weights) + phi*self.engagement_ratio(like, view)

import ast
import itertools
from collections import Counter

tag_rows = top_recent_comments[top_recent_comments['tags'] != '']
tag_col = tag_rows['tags']

tag_col = tag_col.apply(
    lambda x: ast.literal_eval(x) if isinstance(x, str) else x
)

all_tags = list(itertools.chain.from_iterable(tag_col.dropna()))

unique_tags = set(all_tags)
print(f"Sample total tags used: {len(all_tags):,}")
print(f"Sample unique tags: {len(unique_tags):,}")

tag_counts = Counter(all_tags).most_common(20)
print("\nTop 20 most frequent tags in sample:")
for tag, count in tag_counts:
    print(f"{tag}: {count}")

import re

BEAUTY_KEYWORDS = {
    'makeup': ['makeup', 'cosmetics', 'foundation', 'concealer', 'lipstick', 'mascara', 'eyeshadow', 'blush', 'contour', 'primer'],
    'skincare': ['skincare', 'skin', 'serum', 'moisturizer', 'cleanser', 'sunscreen', 'acne', 'anti-aging', 'wrinkle', 'glow'],
    'haircare': ['hair', 'shampoo', 'conditioner', 'haircare', 'color', 'dye', 'treatment', 'styling', 'salon', 'wig'],
    'fragrance': ['perfume', 'fragrance', 'scent', 'cologne', 'eau de toilette', 'eau de parfum'],
    'brands': ['loreal', 'l\'oreal', 'maybelline', 'garnier', 'kerastase', 'lancome', 'ysl', 'urban decay', 'nyx', 'cerave'],
    'tools': ['brush', 'sponge', 'curler', 'straightener', 'blow dryer', 'makeup tools', 'beauty tools']
}

LOREAL_KEYWORDS = [
    'loreal', 'l\'oreal', 'loreal paris', 'loreal professional',
    'loreal makeup', 'loreal skincare', 'loreal hair', 'loreal foundation',
    'loreal lipstick', 'loreal serum', 'loreal shampoo'
]

def categorize_tag(tag):
    tag_lower = tag.lower()

    for loreal_keyword in LOREAL_KEYWORDS:
        if loreal_keyword in tag_lower:
            return 'loreal'

    for category, keywords in BEAUTY_KEYWORDS.items():
        for keyword in keywords:
            if keyword in tag_lower:
                return category
    return 'other'

def is_beauty_related(tag):
    tag_lower = tag.lower()

    if any(loreal_keyword in tag_lower for loreal_keyword in LOREAL_KEYWORDS):
        return True

    for category_keywords in BEAUTY_KEYWORDS.values():
        if any(keyword in tag_lower for keyword in category_keywords):
            return True

    return False

def filter_useless_tags(tags, min_length=3, max_length=50):
    useful_tags = []

    for tag in tags:
        if len(tag) < min_length or len(tag) > max_length:
            continue

        if tag.lower() in ['video', 'shorts', 'youtube', 'viral', 'trending',
                          'tiktok', 'reels', 'instagram', 'facebook', 'social media']:
            continue

        if re.match(r'^[\d\W]+$', tag):
            continue

        useful_tags.append(tag)

    return useful_tags

all_unique_tags = list(unique_tags)

filtered_tags = filter_useless_tags(all_unique_tags)
print(f"After basic filtering: {len(filtered_tags)} tags")

beauty_related_tags = [tag for tag in filtered_tags if is_beauty_related(tag)]
print(f"Beauty-related tags: {len(beauty_related_tags)} tags")

tag_categories = {}
for tag in beauty_related_tags:
    category = categorize_tag(tag)
    if category not in tag_categories:
        tag_categories[category] = []
    tag_categories[category].append(tag)

tags_df = pd.DataFrame({key: pd.Series(value) for key, value in tag_categories.items()})
tags_df.to_csv(r"tag_categories.csv", index=False, encoding="utf-8-sig")

category_counts = {cat: len(tags) for cat, tags in tag_categories.items()}
print("\nðŸ“Š Tag Categories:")
for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
    print(f"{category.upper():<12}: {count:>5} tags")

loreal_tags = [tag for tag in beauty_related_tags if any(keyword in tag.lower() for keyword in LOREAL_KEYWORDS)]
print(f"\nðŸŽ¯ L'OrÃ©al specific tags: {len(loreal_tags)} tags")

# Normalize text for easier matching
def normalize_text(s: str) -> str:
    s = str(s).lower()
    s = re.sub(r"\s+", " ", s).strip()
    return s

top_recent_comments["textOriginal"] = top_recent_comments["textOriginal"].fillna("").map(normalize_text)

# Compile regex for faster matching
category_patterns = {cat: re.compile("|".join(map(re.escape, kws)), flags=re.IGNORECASE)
                     for cat, kws in BEAUTY_KEYWORDS.items()}
loreal_pattern = re.compile("|".join(map(re.escape, LOREAL_KEYWORDS)), flags=re.IGNORECASE)

def detect_categories(text: str):
    cats = [cat for cat, pat in category_patterns.items() if pat.search(text)]
    return cats if cats else None

# Apply to DataFrame
top_recent_comments["categories"] = top_recent_comments["textOriginal"].map(detect_categories)
top_recent_comments["is_loreal"] = top_recent_comments["textOriginal"].str.contains(loreal_pattern)
df_filtered = top_recent_comments[(top_recent_comments["categories"].notna()) | (top_recent_comments["is_loreal"])][["commentId","textOriginal","categories","is_loreal"]]
df_filtered.to_csv(r"filtered_comments.csv", index=False, encoding="utf-8-sig")

print("\nSample with categories:")
print(df_filtered[["textOriginal", "categories", "is_loreal"]])

def process_comments_batch(analyzer, df, batch_size=100, sample_size=None):
    if sample_size:
        df = df.sample(sample_size)

    results = []

    # Process with progress bar
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing comments"):
        try:
            title, comment, like_count, view_count = row['title'], row['textOriginal'], row['videoLikeCount'], row['viewCount']

            # Calculate quality score
            quality_score = analyzer.evaluate(title, comment, like_count, view_count, weights = [0.305, 0.021, 0.145, 0.529], sentiment_weights = [-2, -1, 0, 1, 2])

            # Get individual components for analysis
            title_en = analyzer.translate_to_english(analyzer.deemojize(title))
            comment_en = analyzer.translate_to_english(analyzer.deemojize(comment))
            description_en = analyzer.translate_to_english(analyzer.deemojize(row['description']))

            similarity = analyzer.semantic_similarity(title_en, comment_en)
            spam_prob = analyzer.not_spam_prob(comment_en)
            sentiment = analyzer.sentiment_analysis(comment_en, weights = [-2, -1, 0, 1, 2])
            engagement = analyzer.engagement_ratio(like_count, view_count)

            results.append({
                'commentId': row['commentId'],
                'videoId': row['videoId'],
                'original_comment': comment,
                'video_title': title,
                'translated_comment': comment_en,
                'translated_title': title_en,
                'translated_description': description_en,
                'quality_score': quality_score,
                'semantic_similarity': similarity,
                'spam_probability': spam_prob,
                'sentiment_score': sentiment,
                'engagement_ratio': engagement,
                'videoLikeCount': like_count,
                'viewCount': view_count,
                'tags': row['tags'],
                'contentDuration': row['contentDuration'],
                'quality_true': row['quality']
            })

        except Exception as e:
            print(f"Error processing comment {row.get('commentId', 'unknown')}: {e}")
            continue

    return pd.DataFrame(results)

import random


def main():
    print("Initializing Comment Quality Analyzer...")
    analyzer = CommentQualityAnalyzer()

    print("Processing comments...")
    results_df = process_comments_batch(analyzer, top_recent_comments, sample_size=500)

    generate_report(results_df)

    results_df.to_csv("comment_quality_results.csv", index=False, encoding="utf-8-sig")
    print("\nResults saved to 'comment_quality_results.csv'")


    return results_df

def generate_report(results_df):
    print("\n" + "="*50)
    print("COMMENT QUALITY ANALYSIS REPORT")
    print("="*50)

    print(f"Total comments analyzed: {len(results_df)}")
    print(f"Average quality score: {results_df['quality_score'].mean():.3f}")
    print(f"Median quality score: {results_df['quality_score'].median():.3f}")


    # Categorize comments
    results_df['quality_pred'] = pd.cut(
    results_df['quality_score'],
    bins=[-float('inf'), 0.14, 0.21, float('inf')],
    labels=['bad', 'neutral', 'good'],
    include_lowest = True,
    right = True
)


    print(f"\nQuality Categories:")
    category_counts = results_df['quality_pred'].value_counts()
    for category, count in category_counts.items():
        percentage = (count / len(results_df)) * 100
        print(f"{category}: {count} ({percentage:.1f}%)")

    # Top and bottom comments
    print(f"\nTop 5 Highest Quality Comments:")
    top_comments = results_df.nlargest(5, 'quality_score')
    for _, row in top_comments.iterrows():
        print(f"Score: {row['quality_score']:.2f} - '{row['original_comment'][:50]}...'")

    print(f"\nTop 5 Lowest Quality Comments:")
    bottom_comments = results_df.nsmallest(5, 'quality_score')
    for _, row in bottom_comments.iterrows():
        print(f"Score: {row['quality_score']:.2f} - '{row['original_comment'][:50]}...'")

# Run the analysis
if __name__ == "__main__":
    results = main()

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt


def evaluate_model_performance(results, y_true_col="quality_true", y_pred_col="quality_pred"):
    y_true = results[y_true_col]
    y_pred = results[y_pred_col]

    # Accuracy
    accuracy = accuracy_score(y_true, y_pred)
    print("Accuracy:", round(accuracy, 4))

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, digits=4))

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred, labels=["good", "bad"])
    print("\nConfusion Matrix:\n", cm)

    # Plot Confusion Matrix
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=["Pred: good", "Pred: bad"],
                yticklabels=["True: good", "True: bad"])
    plt.title("Confusion Matrix")
    plt.ylabel("True Label")
    plt.xlabel("Predicted Label")
    plt.show()

    return {
        "accuracy": accuracy,
        "classification_report": classification_report(y_true, y_pred, digits=4, output_dict=True),
        "confusion_matrix": cm
    }
metrics = evaluate_model_performance(results)
metrics

# Additional Features --- Find the suggestive phrases, keywords related to beauty industry and L'Oreal and its brand, in specific
BEAUTY_KEYWORDS = {
    'makeup': ['makeup', 'cosmetics', 'foundation', 'concealer', 'lipstick', 'mascara', 'eyeshadow', 'blush', 'contour', 'primer'],
    'skincare': ['skincare', 'skin', 'serum', 'moisturizer', 'cleanser', 'sunscreen', 'acne', 'anti-aging', 'wrinkle', 'glow'],
    'haircare': ['hair', 'shampoo', 'conditioner', 'haircare', 'color', 'dye', 'treatment', 'styling', 'salon', 'wig'],
    'fragrance': ['perfume', 'fragrance', 'scent', 'cologne', 'eau de toilette', 'eau de parfum'],
    'brands': ['loreal', 'l\'oreal', 'maybelline', 'garnier', 'kerastase', 'lancome', 'ysl', 'urban decay', 'nyx', 'cerave'],
    'tools': ['brush', 'sponge', 'curler', 'straightener', 'blow dryer', 'makeup tools', 'beauty tools']
}

LOREAL_KEYWORDS = [
    'loreal', 'l\'oreal', 'loreal paris', 'loreal professional',
    'loreal makeup', 'loreal skincare', 'loreal hair', 'loreal foundation',
    'loreal lipstick', 'loreal serum', 'loreal shampoo'
]

BRAND_TERMS = [
    "l'oreal","loreal","l oreal","l'oreal paris",
    "elvive","elseve","revitalift","infallible",
    "true match","age perfect","excellence","casting creme gloss",
    "youth code","glycolic bright","dream length","hyaluron moisture",
    "elnett","uv defender",
]

IMPROVE_TERMS = [
    "should","could","would","wish","hope","please","pls","plz","improve","improvement","add","include",
    "launch","release","fix","change","update","upgrade","better","more","less","reduce","increase",
    "shade","range","coverage","transfer","smudge","oxidize","oxidise","fragrance-free","non comedogenic",
    "sensitive","price","cheaper","expensive","afford","packaging","pump","refill"
]

def contains_keyword(text, keywords):
    text = str(text).lower()
    return any(re.search(r"\b" + re.escape(kw.lower()) + r"\b", text) for kw in keywords)

# Flatten beauty keywords into one list
all_beauty_terms = [term for group in BEAUTY_KEYWORDS.values() for term in group]

# âœ… Step 1: Add flags
results["has_improvement_phrase"] = results["translated_comment"].apply(
    lambda x: contains_keyword(x, IMPROVE_TERMS)
)

results["has_loreal_keyword"] = results["translated_comment"].apply(
    lambda x: contains_keyword(x, LOREAL_KEYWORDS + BRAND_TERMS)
)

results["has_beauty_keyword"] = results["translated_comment"].apply(
    lambda x: contains_keyword(x, all_beauty_terms)
)

print("Improvement phrases:", results["has_improvement_phrase"].sum())
print("L'OrÃ©al keywords:", results["has_loreal_keyword"].sum())
print("Beauty keywords:", results["has_beauty_keyword"].sum())

filtered = results[
    results["has_improvement_phrase"] |
    results["has_loreal_keyword"] |
    results["has_beauty_keyword"]
]

results_temp = results.copy()
filtered_temp = filtered.copy()

misclassified = (filtered_temp['quality_pred'] == 'bad') & (filtered_temp['quality_true'] == 'good')
filtered_temp.loc[misclassified, 'quality_pred'] = 'good'

# Align on commentId index
results_temp.set_index('commentId', inplace=True)
filtered_temp.set_index('commentId', inplace=True)

results_temp.update(filtered_temp[['quality_pred']])

# Put commentId back as a column
results_temp.reset_index(inplace=True)
filtered_temp.reset_index(inplace=True)

# Now it's safe to evaluate
metrics = evaluate_model_performance(results_temp)
metrics

results.to_csv('finalized_results.csv', index=False, encoding = 'utf-8-sig')

